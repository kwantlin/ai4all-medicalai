{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuUOjAh-axxo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Example of a convolutional layer\n",
        "conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcX68Lnpni4a"
      },
      "source": [
        "UP SAMPLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npM_1U_qnkco"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Example of up sampling\n",
        "x = torch.randn(1, 16, 32, 32)  # Example input tensor\n",
        "upsampled_x = F.interpolate(x, scale_factor=2, mode='nearest')  # Upsample by a factor of 2 (nearest: duplicating the existing pixels or samples to create new ones)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jP4pqKbOnngt",
        "outputId": "683e563c-2af3-4157-f41a-2ce60aaf91fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[ 1.  2.]\n",
            "  [ 2.  3.]]\n",
            "\n",
            " [[ 4.  5.]\n",
            "  [ 6.  7.]]\n",
            "\n",
            " [[ 8.  9.]\n",
            "  [10. 11.]]] (3, 2, 2)\n",
            "tensor([[[ 1.,  1.,  2.,  2.],\n",
            "         [ 2.,  2.,  3.,  3.]],\n",
            "\n",
            "        [[ 4.,  4.,  5.,  5.],\n",
            "         [ 6.,  6.,  7.,  7.]],\n",
            "\n",
            "        [[ 8.,  8.,  9.,  9.],\n",
            "         [10., 10., 11., 11.]]], dtype=torch.float64) torch.Size([3, 2, 4])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "array = np.array([[[1,2],[2,3]],[[4,5],[6,7]], [[8,9],[10,11]]], dtype=np.float64)\n",
        "print(array, array.shape)\n",
        "interpolated = F.interpolate(torch.from_numpy(array), scale_factor=2, mode='nearest')\n",
        "print(interpolated, interpolated.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E15hIk8n0Vy"
      },
      "source": [
        "DOWNSAMPLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbTvGkx6oGYr",
        "outputId": "875cf597-b856-4f35-fe54-a2ba5b70f6eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 16, 16, 16])\n"
          ]
        }
      ],
      "source": [
        "# Example of downsampling using max pooling\n",
        "max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "downsampled_x = max_pool(x)  # Down sample by a factor of 2\n",
        "print(downsampled_x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sImKC-QoI1u"
      },
      "source": [
        "BATCH NORMALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j1XBENUobKz"
      },
      "outputs": [],
      "source": [
        "# Example of batch normalization\n",
        "batch_norm = nn.BatchNorm2d(num_features=16)\n",
        "normalized_x = batch_norm(upsampled_x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCLLhMc1ohUl"
      },
      "source": [
        "ACTIVATION FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIpmwoFdoio2"
      },
      "outputs": [],
      "source": [
        "# Example of ReLU activation function\n",
        "relu = nn.ReLU()\n",
        "activated_x = relu(normalized_x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMwtTsVrolAW",
        "outputId": "fd3f717c-586d-41b2-cb40-3915432de5c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 16, 64, 64])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "activated_x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pagpMOomopTS"
      },
      "source": [
        "LINEAR LAYERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFlIJaIlosuh"
      },
      "outputs": [],
      "source": [
        "# Example of a linear layer\n",
        "linear_layer = nn.Linear(in_features=65536, out_features=10)\n",
        "output = linear_layer(activated_x.view(activated_x.size(0), -1))  # Flatten the input before feeding into the linear layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbm43xZXowP2"
      },
      "source": [
        "CREATING THE NEURAL NETWORK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo5dNeRuo4Ug"
      },
      "source": [
        "1. DEFINE NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBL3yS5DoxlK",
        "outputId": "bdcfafe6-41c7-4d59-a131-81db100c3e08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from tqdm import tqdm  # For progress bar\n",
        "import random\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import torch.optim as optim\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySmMkG1Po7iJ"
      },
      "source": [
        "2. DEFINE LOSS FUNCTION AND OPTIMIZER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLYPu-wUpDse"
      },
      "source": [
        "3. TRAIN THE NETWORK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_VX-VZgpHX-"
      },
      "source": [
        "4. TEST THE NETWORK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mhZqZmXpopZ"
      },
      "source": [
        "TRAINING WITH OUR DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ncWouJIpXjP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjGJQCSEpsUZ",
        "outputId": "a3f860a7-c7e5-4f7f-aee5-c0815fcfd7e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Use ResNet50\n",
        "model = models.resnet50(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 5)  # Adjust the final layer for 5 classes\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V4BtTFrpvDe"
      },
      "outputs": [],
      "source": [
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform):\n",
        "        self.data_frame = csv_file\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, 'train_images', self.data_frame.iloc[idx, 0] + '.png')\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        label = self.data_frame.iloc[idx, 1]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2mCh83iyE5F"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/drive/My Drive/aptos2019-blindness-detection'\n",
        "csv_file = os.path.join(data_path, 'train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UxOHejIV6Mh"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLoaY8pIWCss"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = df.iloc[:, :]\n",
        "y = df.iloc[:, :]\n",
        "\n",
        "train_set, test_set, ignore1, ignore2 = train_test_split(X, y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kZIx85pp3mj"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation(90),\n",
        "    transforms.RandomApply([\n",
        "        transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1))\n",
        "    ], p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "trainset = APTOSDataset(csv_file = train_set, root_dir = data_path, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=16, shuffle=True, num_workers = 2)\n",
        "\n",
        "validset = APTOSDataset(csv_file = test_set, root_dir = data_path, transform=transform)\n",
        "validloader = DataLoader(validset, batch_size=16, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "aO5NhBlbFnei",
        "outputId": "e3fdc066-478c-4c3b-8a9b-2fdf24765b83"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'full_dataset = APTOSDataset(csv_file=csv_file, root_dir=data_path, transform=transform)\\ntrain_size = int(0.8 * len(full_dataset))  # 80% Train\\nval_size = int(0.1 * len(full_dataset))  # 10% Val\\ntest_size = len(full_dataset) - train_size - val_size  # 10% Test'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''full_dataset = APTOSDataset(csv_file=csv_file, root_dir=data_path, transform=transform)\n",
        "train_size = int(0.8 * len(full_dataset))  # 80% Train\n",
        "val_size = int(0.1 * len(full_dataset))  # 10% Val\n",
        "test_size = len(full_dataset) - train_size - val_size  # 10% Test'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zor9F1fLp8eR",
        "outputId": "4e3b0484-f157-4359-858c-d2df05a2964b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7bc502bba620>\n",
            "Epoch 1, Train Loss: 0.8137676811412625, Train Accuracy: 71.1847046773643%, Valid Loss: 0.7569992172977199, Valid Accuracy: 70.80491132332878%\n",
            "Epoch 2, Train Loss: 0.699427691652723, Train Accuracy: 73.74530556503926%, Valid Loss: 0.6204840651024943, Valid Accuracy: 77.62619372442019%\n",
            "Epoch 3, Train Loss: 0.6496905889362097, Train Accuracy: 76.51075452372824%, Valid Loss: 0.6742133446361708, Valid Accuracy: 73.66984993178717%\n",
            "Epoch 4, Train Loss: 0.6148785328411538, Train Accuracy: 78.01297371116422%, Valid Loss: 0.6920323235833127, Valid Accuracy: 76.26193724420192%\n",
            "Epoch 5, Train Loss: 0.6087981827719056, Train Accuracy: 77.46671218846022%, Valid Loss: 0.5768162517443948, Valid Accuracy: 78.85402455661665%\n",
            "Epoch 6, Train Loss: 0.5917606612100549, Train Accuracy: 77.60327756913622%, Valid Loss: 0.5893783430042474, Valid Accuracy: 80.76398362892223%\n",
            "Epoch 7, Train Loss: 0.5535876311199821, Train Accuracy: 78.93479003072721%, Valid Loss: 0.5619445682867713, Valid Accuracy: 81.44611186903138%\n",
            "Epoch 8, Train Loss: 0.5489797585684321, Train Accuracy: 79.65175827927621%, Valid Loss: 0.6638597628992536, Valid Accuracy: 76.12551159618008%\n",
            "Epoch 9, Train Loss: 0.5587513421702645, Train Accuracy: 79.1737794469102%, Valid Loss: 0.5710528787711392, Valid Accuracy: 78.03547066848567%\n",
            "Epoch 10, Train Loss: 0.5432724531577982, Train Accuracy: 79.20792079207921%, Valid Loss: 0.6187622216732606, Valid Accuracy: 78.58117326057298%\n"
          ]
        }
      ],
      "source": [
        "def train(model, train_loader, valid_loader, criterion, optimizer, epochs=10):\n",
        "    train_losses, valid_losses = [], []\n",
        "    train_accuracies, valid_accuracies = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_accuracy = 100 * correct / total\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        valid_loss, valid_accuracy = validate(model, valid_loader, criterion)\n",
        "        valid_losses.append(valid_loss)\n",
        "        valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss}, Train Accuracy: {train_accuracy}%, Valid Loss: {valid_loss}, Valid Accuracy: {valid_accuracy}%\")\n",
        "\n",
        "    return train_losses, valid_losses, train_accuracies, valid_accuracies\n",
        "\n",
        "def validate(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    valid_loss = running_loss / len(valid_loader)\n",
        "    valid_accuracy = 100 * correct / total\n",
        "    return valid_loss, valid_accuracy\n",
        "\n",
        "print(trainloader)\n",
        "# Train the model\n",
        "train_losses, valid_losses, train_accuracies, valid_accuracies = train(model, trainloader, validloader, criterion, optimizer, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCZ7cEFS1pNj"
      },
      "outputs": [],
      "source": [
        "with tqdm(total=len(train_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\") as pbar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wgaV5pGap_h2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "5f6a45a5-4aee-44c6-af8b-795efa07085a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-923159808ef5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plotting training and validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Valid Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ],
      "source": [
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(valid_losses, label='Valid Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting training and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(valid_accuracies, label='Valid Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpL-Os-_187V"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(test_labels.cpu(), torch.argmax(test_outputs, 1).cpu())\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(5), yticklabels=range(5))\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "def make_confusion_matrix(train_tp, train_tn, train_fp, train_fn, labels=['No', 'Yes']):\n",
        "    # Calculate metrics from TP, TN, FP, FN\n",
        "    cm = np.array([[np.sum(train_tn), np.sum(train_fp)],\n",
        "                   [np.sum(train_fn), np.sum(train_tp)]])\n",
        "    # Create DataFrame for confusion matrix\n",
        "    df_cm = pd.DataFrame(cm, index=[f'Actual - {labels[0]}', f'Actual - {labels[1]}'],\n",
        "                         columns=[f'Predicted - {labels[0]}', f'Predicted - {labels[1]}'])\n",
        "    # Create annotations for each cell\n",
        "    group_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten() / np.sum(cm)]\n",
        "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_counts, group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2, 2)\n",
        "    # Plotting the heatmap with adjusted color scheme\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(df_cm, annot=labels, fmt='', cmap='YlGnBu', cbar=False)  # Change 'cmap' to adjust the color scheme\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.title('Train Confusion Matrix Across All Epochs')\n",
        "    plt.show()\n",
        "# Example usage:\n",
        "make_confusion_matrix(train_tp, train_tn, train_fp, train_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JI-zdVBQ50RR"
      },
      "outputs": [],
      "source": [
        "dummy_input = torch.randn(1, 3, 224, 224)  # Replace 224 with your input size\n",
        "dummy_output = model.conv2(model.pool(F.relu(model.batch_norm1(model.conv1(dummy_input)))))\n",
        "dummy_output = model.pool(F.relu(model.batch_norm2(dummy_output)))\n",
        "print(dummy_output.shape)  # This will help you determine the correct input size for fc1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}