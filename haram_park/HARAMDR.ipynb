{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MuUOjAh-axxo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Example of a convolutional layer\n",
        "conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UP SAMPLING"
      ],
      "metadata": {
        "id": "TcX68Lnpni4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Example of up sampling\n",
        "x = torch.randn(1, 16, 32, 32)  # Example input tensor\n",
        "upsampled_x = F.interpolate(x, scale_factor=2, mode='nearest')  # Upsample by a factor of 2 (nearest: duplicating the existing pixels or samples to create new ones)\n"
      ],
      "metadata": {
        "id": "npM_1U_qnkco"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "array = np.array([[[1,2],[2,3]],[[4,5],[6,7]], [[8,9],[10,11]]], dtype=np.float64)\n",
        "print(array, array.shape)\n",
        "interpolated = F.interpolate(torch.from_numpy(array), scale_factor=2, mode='nearest')\n",
        "print(interpolated, interpolated.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jP4pqKbOnngt",
        "outputId": "6d2d9997-cdb1-4560-8db2-a4dd40335862"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 1.  2.]\n",
            "  [ 2.  3.]]\n",
            "\n",
            " [[ 4.  5.]\n",
            "  [ 6.  7.]]\n",
            "\n",
            " [[ 8.  9.]\n",
            "  [10. 11.]]] (3, 2, 2)\n",
            "tensor([[[ 1.,  1.,  2.,  2.],\n",
            "         [ 2.,  2.,  3.,  3.]],\n",
            "\n",
            "        [[ 4.,  4.,  5.,  5.],\n",
            "         [ 6.,  6.,  7.,  7.]],\n",
            "\n",
            "        [[ 8.,  8.,  9.,  9.],\n",
            "         [10., 10., 11., 11.]]], dtype=torch.float64) torch.Size([3, 2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DOWNSAMPLING"
      ],
      "metadata": {
        "id": "1E15hIk8n0Vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of downsampling using max pooling\n",
        "max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "downsampled_x = max_pool(x)  # Down sample by a factor of 2\n",
        "print(downsampled_x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbTvGkx6oGYr",
        "outputId": "49a994b7-cf82-4884-e2d9-cb0209972976"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 16, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BATCH NORMALIZATION"
      ],
      "metadata": {
        "id": "7sImKC-QoI1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of batch normalization\n",
        "batch_norm = nn.BatchNorm2d(num_features=16)\n",
        "normalized_x = batch_norm(upsampled_x)\n"
      ],
      "metadata": {
        "id": "7j1XBENUobKz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ACTIVATION FUNCTIONS"
      ],
      "metadata": {
        "id": "VCLLhMc1ohUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of ReLU activation function\n",
        "relu = nn.ReLU()\n",
        "activated_x = relu(normalized_x)\n"
      ],
      "metadata": {
        "id": "HIpmwoFdoio2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activated_x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMwtTsVrolAW",
        "outputId": "1d0c548d-d7b6-4712-dbdf-44101325080e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 16, 64, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LINEAR LAYERS"
      ],
      "metadata": {
        "id": "pagpMOomopTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of a linear layer\n",
        "linear_layer = nn.Linear(in_features=65536, out_features=10)\n",
        "output = linear_layer(activated_x.view(activated_x.size(0), -1))  # Flatten the input before feeding into the linear layer"
      ],
      "metadata": {
        "id": "hFlIJaIlosuh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATING THE NEURAL NETWORK"
      ],
      "metadata": {
        "id": "lbm43xZXowP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. DEFINE NETWORK"
      ],
      "metadata": {
        "id": "uo5dNeRuo4Ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from tqdm import tqdm  # For progress bar\n",
        "import random\n",
        "\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, 1, 1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(16)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.batch_norm1((self.conv1(x)))))\n",
        "        print(x.shape)\n",
        "        x = self.pool(F.relu(self.batch_norm2(self.conv2(x))))\n",
        "        print(x.shape)\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleCNN()\n"
      ],
      "metadata": {
        "id": "HBL3yS5DoxlK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e526c5d0-8ce2-476c-ca5a-d63c9ee43974"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. DEFINE LOSS FUNCTION AND OPTIMIZER"
      ],
      "metadata": {
        "id": "ySmMkG1Po7iJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
      ],
      "metadata": {
        "id": "Tjd6PLZaoyyQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. TRAIN THE NETWORK"
      ],
      "metadata": {
        "id": "jLYPu-wUpDse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, criterion, optimizer, epochs=1):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# Example usage (assuming `train_loader` is defined)\n",
        "# train(model, train_loader, criterion, optimizer)\n"
      ],
      "metadata": {
        "id": "uXh5h8s6pGcH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. TEST THE NETWORK"
      ],
      "metadata": {
        "id": "b_VX-VZgpHX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Accuracy: {100 * correct / total}%\")\n",
        "\n",
        "# Example usage (assuming `test_loader` is defined)\n",
        "# test(model, test_loader)\n"
      ],
      "metadata": {
        "id": "s1L1W-KdpLoD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING WITH OUR DATA"
      ],
      "metadata": {
        "id": "-mhZqZmXpopZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "7ncWouJIpXjP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, 1, 1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(16)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.batch_norm1((self.conv1(x)))))\n",
        "        print(x.shape)\n",
        "        x = self.pool(F.relu(self.batch_norm2(self.conv2(x))))\n",
        "        print(x.shape)\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleCNN()"
      ],
      "metadata": {
        "id": "bjGJQCSEpsUZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform):\n",
        "        self.data_frame = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, 'train_images', self.data_frame.iloc[idx, 0] + '.png')\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        label = self.data_frame.iloc[idx, 1]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7V4BtTFrpvDe"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/My Drive/aptos2019-blindness-detection'\n",
        "csv_file = os.path.join(data_path, 'train.csv')"
      ],
      "metadata": {
        "id": "O2mCh83iyE5F"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "trainset = APTOSDataset(csv_file = csv_file, root_dir = data_path, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers = 2)\n",
        "\n",
        "validset = APTOSDataset(csv_file = csv_file, root_dir = data_path, transform=transform)\n",
        "validloader = torch.utils.data.DataLoader(validset, batch_size=64, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "_kZIx85pp3mj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "X5-1C5xWp6Gg"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, valid_loader, criterion, optimizer, epochs=1):\n",
        "    train_losses, valid_losses = [], []\n",
        "    train_accuracies, valid_accuracies = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_accuracy = 100 * correct / total\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        valid_loss, valid_accuracy = validate(model, valid_loader, criterion)\n",
        "        valid_losses.append(valid_loss)\n",
        "        valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss}, Train Accuracy: {train_accuracy}%, Valid Loss: {valid_loss}, Valid Accuracy: {valid_accuracy}%\")\n",
        "\n",
        "    return train_losses, valid_losses, train_accuracies, valid_accuracies\n",
        "\n",
        "def validate(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    valid_loss = running_loss / len(valid_loader)\n",
        "    valid_accuracy = 100 * correct / total\n",
        "    return valid_loss, valid_accuracy\n",
        "\n",
        "# Train the model\n",
        "train_losses, valid_losses, train_accuracies, valid_accuracies = train(model, trainloader, validloader, criterion, optimizer, epochs=5)"
      ],
      "metadata": {
        "id": "Zor9F1fLp8eR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "ab7222fb-5d13-4758-f556-3e6b44d36609"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 16, 112, 112])\n",
            "torch.Size([64, 32, 56, 56])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected input batch_size (4096) to match target batch_size (64).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-5d4444fa2bef>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-5d4444fa2bef>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, valid_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1186\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (4096) to match target batch_size (64)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(valid_losses, label='Valid Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting training and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(valid_accuracies, label='Valid Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wgaV5pGap_h2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}