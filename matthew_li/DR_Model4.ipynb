{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE3ekdo8Mb6k",
        "outputId": "014e6793-5616-4691-98a7-4acaa4db3a0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1DwcsYZWFxUskq7gNIgNqPDzvPfHJSdsh/AI4ALL Diabetic Retinopathy Research/Early ML Model/aptos2019-blindness-detection\n"
          ]
        }
      ],
      "source": [
        "# importing some libraries (not all are currently used)\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from PIL import Image\n",
        "import imageio\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm  # For progress bar\n",
        "\n",
        "\n",
        "# connecting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# change to project directory\n",
        "%cd /content/drive/My\\ Drive/AI4ALL\\ Diabetic\\ Retinopathy\\ Research/Early\\ ML\\ Model/aptos2019-blindness-detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zPuzgPnjMeoO"
      },
      "outputs": [],
      "source": [
        "x_data = np.load('x_train1.npy')\n",
        "y_data = np.load('y_train2.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyEV_RVdId8Y",
        "outputId": "1c479dc0-377c-4ce7-d20c-4188a915e59a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 4, 1, ..., 2, 0, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "y_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6FTyK2sLMhT1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BJtVydQ99_sT"
      },
      "outputs": [],
      "source": [
        "class PreprocessedRetinaDataset(Dataset):\n",
        "    def __init__(self, x_train, y_train, transform=None):\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_train)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image = self.x_train[idx]\n",
        "        label = self.y_train[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224, 224)),  # Resize to match ResNet input size\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2eXve32pMzkn"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, 1, 1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\n",
        "        self.fc1 = nn.Linear(32 * 56 * 56, 128)\n",
        "        self.fc2 = nn.Linear(128, 5)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(16)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.batch_norm1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.batch_norm2(self.conv2(x))))\n",
        "        x = x.view(-1, 32 * 56 * 56)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF5iXAJLNABQ",
        "outputId": "a5db099e-d279-4911-fb3d-ff71c28bdff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create datasets and dataloaders\n",
        "trainset = PreprocessedRetinaDataset(x_train=x_train, y_train=y_train, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=4)\n",
        "\n",
        "validset = PreprocessedRetinaDataset(x_train=x_train, y_train=y_train, transform=transform)\n",
        "validloader = DataLoader(validset, batch_size=64, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Focal Loss\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        loss = (self.alpha[targets] * (1 - pt) ** self.gamma * ce_loss).mean()\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "M6yBvsZ6DEWs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jQsfqhU8BS68"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71CRkLoE-rGy",
        "outputId": "044a6feb-b42d-433a-baf5-b959cdd496f3"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 310.07911491394043 seconds ---\n",
            "Epoch 1: Train Loss: 0.926, Train Accuracy: 65.368%, Valid Loss: 0.816, Valid Accuracy: 69.701%\n",
            "Train auroc: 0.796, Train f1: 0.614%, Valid auroc: 0.874, Valid f1: 0.657%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 302.49212527275085 seconds ---\n",
            "Epoch 2: Train Loss: 0.75, Train Accuracy: 71.413%, Valid Loss: 0.68, Valid Accuracy: 75.273%\n",
            "Train auroc: 0.861, Train f1: 0.673%, Valid auroc: 0.902, Valid f1: 0.704%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "\n",
        "def train(model, train_loader, valid_loader, criterion, optimizer, epochs=1):\n",
        "    train_losses, valid_losses = [], []\n",
        "    train_accuracies, valid_accuracies = [], []\n",
        "    train_f1s, valid_f1s = [], []\n",
        "    train_auc_scores, train_aurocs = [], []\n",
        "    valid_auc_scores, valid_aurocs = [], []\n",
        "    train_tp, train_tn, train_fp, train_fn = [], [], [], []\n",
        "    valid_tp, valid_tn, valid_fp, valid_fn = [], [], [], []\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "        all_train_labels, all_train_preds = [], []\n",
        "        start_time = time.time()\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "            # Collect data for AUROC and F1 score\n",
        "            all_train_labels.extend(labels.cpu().numpy())\n",
        "            all_train_preds.extend(torch.softmax(outputs, dim=1).detach().cpu().numpy())\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_accuracy = 100 * correct / total\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Calculate confusion matrix and TP, TN, FP, FN\n",
        "        cm = confusion_matrix(all_train_labels, np.argmax(all_train_preds, axis=1))\n",
        "        tp = np.diag(cm)\n",
        "        fn = np.sum(cm, axis=1) - tp\n",
        "        fp = np.sum(cm, axis=0) - tp\n",
        "        tn = np.sum(cm) - (tp + fn + fp)\n",
        "        train_tp.append(tp)\n",
        "        train_tn.append(tn)\n",
        "        train_fp.append(fp)\n",
        "        train_fn.append(fn)\n",
        "\n",
        "\n",
        "        # Calculate AUROC and F1 score\n",
        "        all_train_labels = np.array(all_train_labels)\n",
        "        all_train_preds = np.array(all_train_preds)\n",
        "        train_auroc = roc_auc_score(all_train_labels, all_train_preds, multi_class='ovr') if len(np.unique(all_train_labels)) > 1 else float('nan')\n",
        "        train_f1 = f1_score(all_train_labels, np.argmax(all_train_preds, axis=1), average='weighted')\n",
        "\n",
        "        # Add to lists\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        train_aurocs.append(train_auroc)\n",
        "        train_f1s.append(train_f1)\n",
        "\n",
        "        # VALIDATION STEPS\n",
        "\n",
        "        # loss, accuracy, f1s, aurocs\n",
        "        valid_loss, valid_accuracy, valid_auroc, valid_f1, valid_tp_epoch, valid_tn_epoch, valid_fp_epoch, valid_fn_epoch = validate(model, valid_loader, criterion)\n",
        "        valid_losses.append(valid_loss)\n",
        "        valid_accuracies.append(valid_accuracy)\n",
        "        valid_aurocs.append(valid_auroc)\n",
        "        valid_f1s.append(valid_f1)\n",
        "\n",
        "        # tp, tn, fp, fn\n",
        "\n",
        "        valid_tp.append(valid_tp_epoch)\n",
        "        valid_tn.append(valid_tn_epoch)\n",
        "        valid_fp.append(valid_fp_epoch)\n",
        "        valid_fn.append(valid_fn_epoch)\n",
        "\n",
        "\n",
        "        curr = time.ctime(time.time())\n",
        "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "        print(f\"Epoch {epoch+1}: Train Loss: {round(train_loss, 3)}, Train Accuracy: {round(train_accuracy, 3)}%, Valid Loss: {round(valid_loss, 3)}, Valid Accuracy: {round(valid_accuracy, 3)}%\")\n",
        "        print(f\"Train auroc: {round(train_auroc, 3)}, Train f1: {round(train_f1, 3)}%, Valid auroc: {round(valid_auroc, 3)}, Valid f1: {round(valid_f1, 3)}%\")\n",
        "    return (train_losses, valid_losses, train_accuracies, valid_accuracies,\n",
        "            train_aurocs, valid_aurocs,\n",
        "            train_tp, train_tn, train_fp, train_fn,\n",
        "            valid_tp, valid_tn, valid_fp, valid_fn)\n",
        "\n",
        "def validate(model, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "    all_valid_labels, all_valid_preds = [], []\n",
        "    all_valid_labels, all_valid_preds = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Collect data for AUROC and F1 score\n",
        "            all_valid_labels.extend(labels.cpu().numpy())\n",
        "            all_valid_preds.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    valid_loss = running_loss / len(valid_loader)\n",
        "    valid_accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate AUROC and F1 score\n",
        "    all_valid_labels = np.array(all_valid_labels)\n",
        "    all_valid_preds = np.array(all_valid_preds)\n",
        "    valid_f1 = f1_score(all_valid_labels, np.argmax(all_valid_preds, axis=1), average='weighted')\n",
        "    valid_auroc = roc_auc_score(all_valid_labels, all_valid_preds, multi_class='ovr') if len(np.unique(all_valid_labels)) > 1 else float('nan')\n",
        "\n",
        "\n",
        "    # Calculate confusion matrix and TP, TN, FP, FN\n",
        "    cm = confusion_matrix(all_valid_labels, np.argmax(all_valid_preds, axis=1))\n",
        "    tp = np.diag(cm)\n",
        "    fn = np.sum(cm, axis=1) - tp\n",
        "    fp = np.sum(cm, axis=0) - tp\n",
        "    tn = np.sum(cm) - (tp + fn + fp)\n",
        "\n",
        "    return valid_loss, valid_accuracy, valid_auroc, valid_f1, tp, tn, fp, fn\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_losses, valid_losses, train_accuracies, valid_accuracies, train_aurocs, valid_aurocs, train_tp, train_tn, train_fp, train_fn, valid_tp, valid_tn, valid_fp, valid_fn = train(model, trainloader, validloader, criterion, optimizer, epochs=30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_384_m9GmTI"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'Model_CNN_V4_Matthew_99%.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q5GCo9ikxbT"
      },
      "outputs": [],
      "source": [
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(valid_losses, label='Valid Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting training and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(valid_accuracies, label='Valid Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "to_jd_HAXv7w"
      },
      "outputs": [],
      "source": [
        "# Plotting training and validation loss\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(valid_losses, label='Valid Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting training and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(valid_accuracies, label='Valid Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7nmZK2OZgnb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def make_confusion_matrix(train_tp, train_tn, train_fp, train_fn, labels=['No', 'Yes']):\n",
        "    # Calculate metrics from TP, TN, FP, FN\n",
        "    cm = np.array([[np.sum(train_tn), np.sum(train_fp)],\n",
        "                   [np.sum(train_fn), np.sum(train_tp)]])\n",
        "\n",
        "    # Create DataFrame for confusion matrix\n",
        "    df_cm = pd.DataFrame(cm, index=[f'Actual - {labels[0]}', f'Actual - {labels[1]}'],\n",
        "                         columns=[f'Predicted - {labels[0]}', f'Predicted - {labels[1]}'])\n",
        "\n",
        "    # Create annotations for each cell\n",
        "    group_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten() / np.sum(cm)]\n",
        "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_counts, group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2, 2)\n",
        "\n",
        "    # Plotting the heatmap with adjusted color scheme\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(df_cm, annot=labels, fmt='', cmap='YlGnBu', cbar=False)  # Change 'cmap' to adjust the color scheme\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.title('Train Confusion Matrix Across All Epochs')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "make_confusion_matrix(train_tp, train_tn, train_fp, train_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hL8O0Xt1Zkvw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def make_confusion_matrix(train_tp, train_tn, train_fp, train_fn, labels=['No', 'Yes']):\n",
        "    # Calculate metrics from TP, TN, FP, FN\n",
        "    cm = np.array([[np.sum(train_tn), np.sum(train_fp)],\n",
        "                   [np.sum(train_fn), np.sum(train_tp)]])\n",
        "\n",
        "    # Create DataFrame for confusion matrix\n",
        "    df_cm = pd.DataFrame(cm, index=[f'Actual - {labels[0]}', f'Actual - {labels[1]}'],\n",
        "                         columns=[f'Predicted - {labels[0]}', f'Predicted - {labels[1]}'])\n",
        "\n",
        "    # Create annotations for each cell\n",
        "    group_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten() / np.sum(cm)]\n",
        "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_counts, group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2, 2)\n",
        "\n",
        "    # Plotting the heatmap with adjusted color scheme\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(df_cm, annot=labels, fmt='', cmap='YlGnBu', cbar=False)  # Change 'cmap' to adjust the color scheme\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.title('Valid Confusion Matrix Across All Epochs')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "make_confusion_matrix(valid_tp, valid_tn, valid_fp, valid_fn)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}